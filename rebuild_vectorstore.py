"""
rebuild_vectorstore.py
Reconstruye el vectorstore con soporte para Markdown y PDFs
Optimizado para documentos bancarios estructurados
"""

import os
import shutil
from langchain_community.document_loaders import UnstructuredMarkdownLoader, PyPDFLoader
from langchain_text_splitters import (
    MarkdownHeaderTextSplitter, 
    RecursiveCharacterTextSplitter
)
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from dotenv import load_dotenv
from typing import List
from langchain_core.documents import Document

# ================================
# CONFIGURACIÃ“N
# ================================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY no estÃ¡ definida en las variables de entorno")

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

VECTORSTORE_PATH = "./vectorstore"
COLLECTION_NAME = "banco_rag"
DOCS_FOLDER = "./docs"  # Carpeta para Markdown y PDFs

# ================================
# CONFIGURACIÃ“N DE CHUNKING
# ================================
# Para Markdown: dividir por headers primero
HEADERS_TO_SPLIT = [
    ("#", "seccion"),
    ("##", "subseccion"),
    ("###", "topico"),
]

# ConfiguraciÃ³n de chunking secundario
CHUNK_SIZE = 800  # Tokens aproximados
CHUNK_OVERLAP = 150  # Overlap para mantener contexto

# ================================
# FUNCIONES DE CARGA
# ================================
def load_markdown_files() -> List[Document]:
    """Carga archivos Markdown con chunking semÃ¡ntico por headers"""
    print("\nğŸ“„ Procesando archivos Markdown...")
    docs = []
    
    md_files = [f for f in os.listdir(DOCS_FOLDER) if f.lower().endswith(".md")]
    
    if not md_files:
        print("  âš  No se encontraron archivos .md")
        return docs
    
    # Splitter por headers de Markdown
    md_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=HEADERS_TO_SPLIT,
        strip_headers=False  # Mantener headers en el contenido
    )
    
    # Splitter secundario para chunks grandes
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n## ", "\n### ", "\n#### ", "\n\n", "\n", " "],
        length_function=len,
    )
    
    for file in md_files:
        path = os.path.join(DOCS_FOLDER, file)
        print(f"  ğŸ“ Procesando: {file}")
        
        try:
            # Leer contenido
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Split por headers (preserva metadata de secciones)
            header_splits = md_splitter.split_text(content)
            
            # Split adicional si los chunks son muy grandes
            final_chunks = []
            for doc in header_splits:
                if len(doc.page_content) > CHUNK_SIZE * 1.5:
                    # Chunk es muy grande, dividir mÃ¡s
                    sub_chunks = text_splitter.split_documents([doc])
                    final_chunks.extend(sub_chunks)
                else:
                    final_chunks.append(doc)
            
            # Agregar metadata de origen
            for chunk in final_chunks:
                chunk.metadata["source"] = path
                chunk.metadata["file_type"] = "markdown"
                chunk.metadata["source_file"] = file
            
            docs.extend(final_chunks)
            print(f"    âœ“ {len(final_chunks)} chunks creados")
            
        except Exception as e:
            print(f"    âœ— Error procesando {file}: {e}")
    
    return docs

def load_pdf_files() -> List[Document]:
    """Carga archivos PDF con chunking tradicional"""
    print("\nğŸ“• Procesando archivos PDF...")
    docs = []
    
    pdf_files = [f for f in os.listdir(DOCS_FOLDER) if f.lower().endswith(".pdf")]
    
    if not pdf_files:
        print("  âš  No se encontraron archivos .pdf")
        return docs
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""],
        length_function=len,
    )
    
    for file in pdf_files:
        path = os.path.join(DOCS_FOLDER, file)
        print(f"  ğŸ“„ Procesando: {file}")
        
        try:
            loader = PyPDFLoader(path)
            raw_docs = loader.load()
            chunks = text_splitter.split_documents(raw_docs)
            
            # Agregar metadata
            for chunk in chunks:
                chunk.metadata["source"] = path
                chunk.metadata["file_type"] = "pdf"
                chunk.metadata["source_file"] = file
            
            docs.extend(chunks)
            print(f"    âœ“ {len(chunks)} chunks creados")
            
        except Exception as e:
            print(f"    âœ— Error procesando {file}: {e}")
    
    return docs

# ================================
# FUNCIÃ“N PRINCIPAL
# ================================
def rebuild_vectorstore():
    """Reconstruye completamente el vectorstore"""
    print("\n" + "="*60)
    print("ğŸ”„ RECONSTRUCCIÃ“N DE VECTORSTORE")
    print("="*60)
    
    # Verificar carpeta de documentos
    if not os.path.exists(DOCS_FOLDER):
        print(f"\nâŒ Error: La carpeta '{DOCS_FOLDER}' no existe")
        print(f"   Crea la carpeta y agrega tus archivos .md y/o .pdf")
        return
    
    files = [f for f in os.listdir(DOCS_FOLDER) if f.endswith(('.md', '.pdf'))]
    if not files:
        print(f"\nâŒ Error: No hay archivos .md o .pdf en '{DOCS_FOLDER}'")
        return
    
    print(f"\nğŸ“‚ Archivos encontrados: {len(files)}")
    for f in files:
        print(f"   â€¢ {f}")
    
    # Confirmar reconstrucciÃ³n
    print("\nâš ï¸  ADVERTENCIA: Esto BORRARÃ el vectorstore actual.")
    confirm = input("Â¿EstÃ¡s seguro de continuar? (s/N): ")
    if confirm.lower() != "s":
        print("âŒ OperaciÃ³n cancelada.")
        return
    
    # 1. Borrar vectorstore existente
    if os.path.exists(VECTORSTORE_PATH):
        print(f"\nğŸ—‘ï¸  Borrando vectorstore existente: {VECTORSTORE_PATH}")
        shutil.rmtree(VECTORSTORE_PATH)
    
    # 2. Crear directorios necesarios
    os.makedirs(VECTORSTORE_PATH, exist_ok=True)
    
    # 3. Cargar documentos
    all_docs = []
    
    # Cargar Markdown (prioritario)
    md_docs = load_markdown_files()
    all_docs.extend(md_docs)
    
    # Cargar PDFs (si existen)
    pdf_docs = load_pdf_files()
    all_docs.extend(pdf_docs)
    
    if not all_docs:
        print("\nâŒ No se pudieron procesar documentos.")
        return
    
    # 4. EstadÃ­sticas
    print(f"\nğŸ“Š RESUMEN:")
    print(f"   â€¢ Total de chunks: {len(all_docs)}")
    print(f"   â€¢ Chunks de Markdown: {len(md_docs)}")
    print(f"   â€¢ Chunks de PDF: {len(pdf_docs)}")
    
    # EstadÃ­sticas de metadata
    sections = set()
    for doc in all_docs:
        if "seccion" in doc.metadata:
            sections.add(doc.metadata["seccion"])
    
    if sections:
        print(f"   â€¢ Secciones identificadas: {len(sections)}")
    
    # 5. Crear embeddings y vectorstore
    print(f"\nğŸ”§ Creando embeddings con OpenAI...")
    print(f"   Modelo: text-embedding-3-small")
    
    try:
        embedding = OpenAIEmbeddings(model="text-embedding-3-small")
        
        print(f"   Creando vectorstore en: {VECTORSTORE_PATH}")
        vectorstore = Chroma.from_documents(
            documents=all_docs,
            embedding=embedding,
            collection_name=COLLECTION_NAME,
            persist_directory=VECTORSTORE_PATH
        )
        
        print(f"\nâœ… VECTORSTORE RECONSTRUIDO EXITOSAMENTE")
        print(f"   ğŸ“ UbicaciÃ³n: {VECTORSTORE_PATH}")
        print(f"   ğŸ“¦ Collection: {COLLECTION_NAME}")
        print(f"   ğŸ“ˆ Total chunks indexados: {len(all_docs)}")
        
        # VerificaciÃ³n
        collection_count = vectorstore._collection.count()
        print(f"   âœ“ Verificado en Chroma: {collection_count} documentos")
        
    except Exception as e:
        print(f"\nâŒ Error creando vectorstore: {e}")
        return
    
    print("\n" + "="*60)
    print("ğŸ‰ Proceso completado. Puedes iniciar el servidor con:")
    print("   python app.py")
    print("="*60 + "\n")

# ================================
# FUNCIÃ“N DE INSPECCIÃ“N
# ================================
def inspect_vectorstore():
    """Inspecciona el vectorstore actual sin modificarlo"""
    print("\n" + "="*60)
    print("ğŸ” INSPECCIÃ“N DE VECTORSTORE")
    print("="*60)
    
    if not os.path.exists(VECTORSTORE_PATH):
        print("\nâŒ Vectorstore no existe")
        return
    
    try:
        embedding = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = Chroma(
            persist_directory=VECTORSTORE_PATH,
            embedding_function=embedding,
            collection_name=COLLECTION_NAME
        )
        
        collection = vectorstore._collection
        count = collection.count()
        
        print(f"\nğŸ“Š ESTADÃSTICAS:")
        print(f"   â€¢ Total de chunks: {count}")
        print(f"   â€¢ Collection: {COLLECTION_NAME}")
        print(f"   â€¢ UbicaciÃ³n: {VECTORSTORE_PATH}")
        
        # Obtener una muestra
        if count > 0:
            sample = collection.peek(limit=5)
            print(f"\nğŸ“ MUESTRA DE DOCUMENTOS (primeros 5):")
            for i, doc in enumerate(sample['documents'][:5], 1):
                metadata = sample['metadatas'][i-1] if i-1 < len(sample['metadatas']) else {}
                print(f"\n   [{i}] {metadata.get('source_file', 'N/A')}")
                print(f"       SecciÃ³n: {metadata.get('seccion', 'N/A')}")
                print(f"       Preview: {doc[:100]}...")
        
    except Exception as e:
        print(f"\nâŒ Error inspeccionando: {e}")
    
    print("\n" + "="*60 + "\n")

# ================================
# EJECUTAR
# ================================
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--inspect":
        inspect_vectorstore()
    else:
        rebuild_vectorstore()
