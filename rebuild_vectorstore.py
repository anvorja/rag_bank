# """
# rebuild_vectorstore.py
# Reconstruye el vectorstore con soporte para Markdown y PDFs
# Optimizado para documentos bancarios estructurados
# """
#
# import os
# import shutil
# from langchain_community.document_loaders import UnstructuredMarkdownLoader, PyPDFLoader
# from langchain_text_splitters import (
#     MarkdownHeaderTextSplitter,
#     RecursiveCharacterTextSplitter
# )
# from langchain_openai import OpenAIEmbeddings
# from langchain_chroma import Chroma
# from dotenv import load_dotenv
# from typing import List
# from langchain_core.documents import Document
#
# # ================================
# # CONFIGURACI√ìN
# # ================================
# load_dotenv()
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# if not OPENAI_API_KEY:
#     raise ValueError("OPENAI_API_KEY no est√° definida en las variables de entorno")
#
# os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
#
# VECTORSTORE_PATH = "./vectorstore"
# COLLECTION_NAME = "banco_rag"
# DOCS_FOLDER = "./docs"  # Carpeta para Markdown y PDFs
#
# # ================================
# # CONFIGURACI√ìN DE CHUNKING
# # ================================
# # Para Markdown: dividir por headers primero
# HEADERS_TO_SPLIT = [
#     ("#", "seccion"),
#     ("##", "subseccion"),
#     ("###", "topico"),
# ]
#
# # Configuraci√≥n de chunking secundario
# CHUNK_SIZE = 800  # Tokens aproximados
# CHUNK_OVERLAP = 150  # Overlap para mantener contexto
#
# # ================================
# # FUNCIONES DE CARGA
# # ================================
# def load_markdown_files() -> List[Document]:
#     """Carga archivos Markdown con chunking sem√°ntico por headers"""
#     print("\nüìÑ Procesando archivos Markdown...")
#     docs = []
#
#     md_files = [f for f in os.listdir(DOCS_FOLDER) if f.lower().endswith(".md")]
#
#     if not md_files:
#         print("  ‚ö† No se encontraron archivos .md")
#         return docs
#
#     # Splitter por headers de Markdown
#     md_splitter = MarkdownHeaderTextSplitter(
#         headers_to_split_on=HEADERS_TO_SPLIT,
#         strip_headers=False  # Mantener headers en el contenido
#     )
#
#     # Splitter secundario para chunks grandes
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=CHUNK_SIZE,
#         chunk_overlap=CHUNK_OVERLAP,
#         separators=["\n## ", "\n### ", "\n#### ", "\n\n", "\n", " "],
#         length_function=len,
#     )
#
#     for file in md_files:
#         path = os.path.join(DOCS_FOLDER, file)
#         print(f"  üìù Procesando: {file}")
#
#         try:
#             # Leer contenido
#             with open(path, 'r', encoding='utf-8') as f:
#                 content = f.read()
#
#             # Split por headers (preserva metadata de secciones)
#             header_splits = md_splitter.split_text(content)
#
#             # Split adicional si los chunks son muy grandes
#             final_chunks = []
#             for doc in header_splits:
#                 if len(doc.page_content) > CHUNK_SIZE * 1.5:
#                     # Chunk es muy grande, dividir m√°s
#                     sub_chunks = text_splitter.split_documents([doc])
#                     final_chunks.extend(sub_chunks)
#                 else:
#                     final_chunks.append(doc)
#
#             # Agregar metadata de origen
#             for chunk in final_chunks:
#                 chunk.metadata["source"] = path
#                 chunk.metadata["file_type"] = "markdown"
#                 chunk.metadata["source_file"] = file
#
#             docs.extend(final_chunks)
#             print(f"    ‚úì {len(final_chunks)} chunks creados")
#
#         except Exception as e:
#             print(f"    ‚úó Error procesando {file}: {e}")
#
#     return docs
#
# def load_pdf_files() -> List[Document]:
#     """Carga archivos PDF con chunking tradicional"""
#     print("\nüìï Procesando archivos PDF...")
#     docs = []
#
#     pdf_files = [f for f in os.listdir(DOCS_FOLDER) if f.lower().endswith(".pdf")]
#
#     if not pdf_files:
#         print("  ‚ö† No se encontraron archivos .pdf")
#         return docs
#
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=CHUNK_SIZE,
#         chunk_overlap=CHUNK_OVERLAP,
#         separators=["\n\n", "\n", " ", ""],
#         length_function=len,
#     )
#
#     for file in pdf_files:
#         path = os.path.join(DOCS_FOLDER, file)
#         print(f"  üìÑ Procesando: {file}")
#
#         try:
#             loader = PyPDFLoader(path)
#             raw_docs = loader.load()
#             chunks = text_splitter.split_documents(raw_docs)
#
#             # Agregar metadata
#             for chunk in chunks:
#                 chunk.metadata["source"] = path
#                 chunk.metadata["file_type"] = "pdf"
#                 chunk.metadata["source_file"] = file
#
#             docs.extend(chunks)
#             print(f"    ‚úì {len(chunks)} chunks creados")
#
#         except Exception as e:
#             print(f"    ‚úó Error procesando {file}: {e}")
#
#     return docs
#
# # ================================
# # FUNCI√ìN PRINCIPAL
# # ================================
# def rebuild_vectorstore():
#     """Reconstruye completamente el vectorstore"""
#     print("\n" + "="*60)
#     print("üîÑ RECONSTRUCCI√ìN DE VECTORSTORE")
#     print("="*60)
#
#     # Verificar carpeta de documentos
#     if not os.path.exists(DOCS_FOLDER):
#         print(f"\n‚ùå Error: La carpeta '{DOCS_FOLDER}' no existe")
#         print(f"   Crea la carpeta y agrega tus archivos .md y/o .pdf")
#         return
#
#     files = [f for f in os.listdir(DOCS_FOLDER) if f.endswith(('.md', '.pdf'))]
#     if not files:
#         print(f"\n‚ùå Error: No hay archivos .md o .pdf en '{DOCS_FOLDER}'")
#         return
#
#     print(f"\nüìÇ Archivos encontrados: {len(files)}")
#     for f in files:
#         print(f"   ‚Ä¢ {f}")
#
#     # Confirmar reconstrucci√≥n
#     print("\n‚ö†Ô∏è  ADVERTENCIA: Esto BORRAR√Å el vectorstore actual.")
#     confirm = input("¬øEst√°s seguro de continuar? (s/N): ")
#     if confirm.lower() != "s":
#         print("‚ùå Operaci√≥n cancelada.")
#         return
#
#     # 1. Borrar vectorstore existente
#     if os.path.exists(VECTORSTORE_PATH):
#         print(f"\nüóëÔ∏è  Borrando vectorstore existente: {VECTORSTORE_PATH}")
#         shutil.rmtree(VECTORSTORE_PATH)
#
#     # 2. Crear directorios necesarios
#     os.makedirs(VECTORSTORE_PATH, exist_ok=True)
#
#     # 3. Cargar documentos
#     all_docs = []
#
#     # Cargar Markdown (prioritario)
#     md_docs = load_markdown_files()
#     all_docs.extend(md_docs)
#
#     # Cargar PDFs (si existen)
#     pdf_docs = load_pdf_files()
#     all_docs.extend(pdf_docs)
#
#     if not all_docs:
#         print("\n‚ùå No se pudieron procesar documentos.")
#         return
#
#     # 4. Estad√≠sticas
#     print(f"\nüìä RESUMEN:")
#     print(f"   ‚Ä¢ Total de chunks: {len(all_docs)}")
#     print(f"   ‚Ä¢ Chunks de Markdown: {len(md_docs)}")
#     print(f"   ‚Ä¢ Chunks de PDF: {len(pdf_docs)}")
#
#     # Estad√≠sticas de metadata
#     sections = set()
#     for doc in all_docs:
#         if "seccion" in doc.metadata:
#             sections.add(doc.metadata["seccion"])
#
#     if sections:
#         print(f"   ‚Ä¢ Secciones identificadas: {len(sections)}")
#
#     # 5. Crear embeddings y vectorstore
#     print(f"\nüîß Creando embeddings con OpenAI...")
#     print(f"   Modelo: text-embedding-3-small")
#
#     try:
#         embedding = OpenAIEmbeddings(model="text-embedding-3-small")
#
#         print(f"   Creando vectorstore en: {VECTORSTORE_PATH}")
#         vectorstore = Chroma.from_documents(
#             documents=all_docs,
#             embedding=embedding,
#             collection_name=COLLECTION_NAME,
#             persist_directory=VECTORSTORE_PATH
#         )
#
#         print(f"\n‚úÖ VECTORSTORE RECONSTRUIDO EXITOSAMENTE")
#         print(f"   üìç Ubicaci√≥n: {VECTORSTORE_PATH}")
#         print(f"   üì¶ Collection: {COLLECTION_NAME}")
#         print(f"   üìà Total chunks indexados: {len(all_docs)}")
#
#         # Verificaci√≥n
#         collection_count = vectorstore._collection.count()
#         print(f"   ‚úì Verificado en Chroma: {collection_count} documentos")
#
#     except Exception as e:
#         print(f"\n‚ùå Error creando vectorstore: {e}")
#         return
#
#     print("\n" + "="*60)
#     print("üéâ Proceso completado. Puedes iniciar el servidor con:")
#     print("   python app.py")
#     print("="*60 + "\n")
#
# # ================================
# # FUNCI√ìN DE INSPECCI√ìN
# # ================================
# def inspect_vectorstore():
#     """Inspecciona el vectorstore actual sin modificarlo"""
#     print("\n" + "="*60)
#     print("üîç INSPECCI√ìN DE VECTORSTORE")
#     print("="*60)
#
#     if not os.path.exists(VECTORSTORE_PATH):
#         print("\n‚ùå Vectorstore no existe")
#         return
#
#     try:
#         embedding = OpenAIEmbeddings(model="text-embedding-3-small")
#         vectorstore = Chroma(
#             persist_directory=VECTORSTORE_PATH,
#             embedding_function=embedding,
#             collection_name=COLLECTION_NAME
#         )
#
#         collection = vectorstore._collection
#         count = collection.count()
#
#         print(f"\nüìä ESTAD√çSTICAS:")
#         print(f"   ‚Ä¢ Total de chunks: {count}")
#         print(f"   ‚Ä¢ Collection: {COLLECTION_NAME}")
#         print(f"   ‚Ä¢ Ubicaci√≥n: {VECTORSTORE_PATH}")
#
#         # Obtener una muestra
#         if count > 0:
#             sample = collection.peek(limit=5)
#             print(f"\nüìù MUESTRA DE DOCUMENTOS (primeros 5):")
#             for i, doc in enumerate(sample['documents'][:5], 1):
#                 metadata = sample['metadatas'][i-1] if i-1 < len(sample['metadatas']) else {}
#                 print(f"\n   [{i}] {metadata.get('source_file', 'N/A')}")
#                 print(f"       Secci√≥n: {metadata.get('seccion', 'N/A')}")
#                 print(f"       Preview: {doc[:100]}...")
#
#     except Exception as e:
#         print(f"\n‚ùå Error inspeccionando: {e}")
#
#     print("\n" + "="*60 + "\n")
#
# # ================================
# # EJECUTAR
# # ================================
# if __name__ == "__main__":
#     import sys
#
#     if len(sys.argv) > 1 and sys.argv[1] == "--inspect":
#         inspect_vectorstore()
#     else:
#         rebuild_vectorstore()


# v2
# TODO: mejorar de tal forma que
# rebuild_vectorstore.py
# Reconstruya el vectorstore con soporte para Markdown y PDFs
# Optimizado para documentos bancarios estructurados similar a V!
from pathlib import Path
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from rag.embeddings import get_embeddings
from rag.vectorstore import get_vectorstore
from config.settings import settings


def extract_metadata_from_markdown(content: str, headers: dict) -> dict:
    """Extrae metadata estructurada del markdown bancario"""
    metadata = {"secci√≥n": "General", "subsecci√≥n": "N/A", "categoria": "N/A"}

    # Mapeo de headers a metadata
    if "H1" in headers:
        metadata["secci√≥n"] = headers["H1"]
    if "H2" in headers:
        metadata["subsecci√≥n"] = headers["H2"]
    if "H3" in headers:
        metadata["categoria"] = headers["H3"]

    # Extraer palabras clave del contenido
    keywords = ["cuenta", "cr√©dito", "tarjeta", "seguro", "inversi√≥n"]
    found_keywords = [kw for kw in keywords if kw.lower() in content.lower()]
    metadata["palabras_clave"] = ", ".join(found_keywords)

    return metadata


def build_vectorstore():
    """Construye el vectorstore desde cero con metadata enriquecida"""
    print("üîç Cargando documentos...")
    docs_path = Path(settings.DOCS_FOLDER)
    if not docs_path.exists():
        raise FileNotFoundError(f"No existe la carpeta {settings.DOCS_FOLDER}")

    all_docs = []
    for md_file in docs_path.glob("*.md"):
        loader = UnstructuredMarkdownLoader(str(md_file))
        documents = loader.load()

        # Split por headers de markdown
        headers_to_split_on = [
            ("#", "H1"),
            ("##", "H2"),
            ("###", "H3"),
        ]
        header_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on,
            strip_headers=False
        )
        header_docs = header_splitter.split_text(documents[0].page_content)

        # Split adicional por tama√±o
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # Tama√±o optimizado para documentos densos
            chunk_overlap=120,  # Overlap para mantener contexto
            separators=["\n\n", "\n", ". ", " ", ""]
        )

        for doc in header_docs:
            # Extraer metadata de headers
            metadata = extract_metadata_from_markdown(
                doc.page_content,
                doc.metadata
            )
            metadata["source"] = md_file.name

            # Split en chunks m√°s peque√±os
            chunks = text_splitter.split_text(doc.page_content)

            for i, chunk in enumerate(chunks):
                chunk_metadata = metadata.copy()
                chunk_metadata["chunk_id"] = f"{md_file.stem}_{i}"
                all_docs.append(Document(page_content=chunk, metadata=chunk_metadata))

    print(f"‚úÖ {len(all_docs)} chunks creados")

    # Crear embeddings y vectorstore
    print("üß† Generando embeddings locales...")
    embeddings = get_embeddings()
    vectorstore = get_vectorstore(recreate=True)

    # A√±adir documentos con batching para memoria limitada
    batch_size = 100
    for i in range(0, len(all_docs), batch_size):
        batch = all_docs[i:i + batch_size]
        vectorstore.add_documents(batch)
        print(f"  ‚Üí Procesados {min(i + batch_size, len(all_docs))} chunks...")

    print(f"‚úÖ Vectorstore creado en {settings.VECTORSTORE_PATH}")


if __name__ == "__main__":
    build_vectorstore()